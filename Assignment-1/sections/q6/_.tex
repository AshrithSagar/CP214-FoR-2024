\section*{Problem 6}

For each equation below state whether:
\begin{itemize}
    \item The equation is nonsense. If so, why?
    \item Is always true. Why?
    \item Is never true. Why?
    \item Is sometimes true. Why?
\end{itemize}

\begin{enumerate}[label = (\alph*)]
    \item \( \vec{b} \times \vec{c} = \vec{c} \times \vec{b} \)
    \item \( \vec{b} \times \vec{c} = \vec{c} \cdot \vec{b} \)
    \item \( \vec{c} \cdot (\vec{a} \times \vec{b}) = \vec{b} \cdot (\vec{c} \times \vec{a}) \)
    \item \( \vec{a} \times (\vec{b} \times \vec{c}) = (\vec{a} \cdot \vec{c})\ \vec{b} - (\vec{a} \cdot \vec{b})\ \vec{c} \)
\end{enumerate}
where: \( \cdot \) = dot product and \( \times \) = cross product by definition.

\subsection*{Solution}

\subsubsection*{(a) \( \vec{b} \times \vec{c} = \vec{c} \times \vec{b} \)}

The equation is \underline{sometimes true}.

The cross product of two vectors is not commutative in general, i.e., \( \vec{a} \times \vec{b} \neq \vec{b} \times \vec{a} \).
In fact, the cross product of two vectors is anti-commutative, i.e., \( \vec{a} \times \vec{b} = -\vec{b} \times \vec{a} \).

The equation is only valid when the two vectors are parallel or anti-parallel, or the vector is the zero vector.
Therefore, the equation is sometimes true.

\subsubsection*{(b) \( \vec{b} \times \vec{c} = \vec{c} \cdot \vec{b} \)}

The equation is \underline{nonsense}.

The LHS of the equation is a vector due to the cross product.
The RHS of the equation is a scalar due to the dot product.

The cross product of two vectors results in a vector that is orthogonal to the plane formed by the two vectors.
The dot product of two vectors results in a scalar that is the projection of one vector onto the other vector.

Therefore, it makes no sense to use them like this.

\subsubsection*{(c) \( \vec{c} \cdot (\vec{a} \times \vec{b}) = \vec{b} \cdot (\vec{c} \times \vec{a}) \)}

This equation is \underline{always true}.

The quantity \( \vec{a} \cdot (\vec{b} \times \vec{c}) \) denotes the \textit{scalar triple product}.
Scalar triple products have the property that they are invariant to cyclic shifts, as above.

We can show this in general, by first proving a trivial lemma.

\textbf{Lemma:} For any three vectors \( \vec{a} \), \( \vec{b} \), and \( \vec{c} \), the following identity holds:
\begin{equation*}
    \vec{p} \cdot (\vec{p} \times \vec{q}) = 0
\end{equation*}

\textbf{Proof:}
The cross product of two vectors is orthogonal to the plane formed by the two vectors, i.e., the \( \vec{p} \times \vec{q} \) is orthogonal to \( \vec{p} \).
Since the dot product of a vector with an orthogonal vector to it is zero, the result follows.

Now, we consider two vectors \( \vec{A} \) and \( \vec{B} \), where \( \vec{A} = (\vec{b} + \vec{c}) \) and \( \vec{B} = ((\vec{b} + \vec{c}) \times \vec{a}) \).
By the above lemma, we see that \( \vec{A} \cdot \vec{B} = 0 \) by noting that \( \vec{B} = \vec{A} \times \vec{a} \).
\begin{equation*}
    (\vec{b} + \vec{c}) \cdot ((\vec{b} + \vec{c}) \times \vec{a}) = 0
\end{equation*}
Expanding this expression, using the associative property, we get
\[
    (\vec{b} + \vec{c}) \cdot (\vec{b} \times \vec{a} + \vec{c} \times \vec{a}) = 0
\]
\[
    \implies \vec{b} \cdot (\vec{b} \times \vec{a}) + \vec{b} \cdot (\vec{c} \times \vec{a}) + \vec{c} \cdot (\vec{b} \times \vec{a}) + \vec{c} \cdot (\vec{c} \times \vec{a}) = 0
\]
Using the above lemma, we see that the first and fourth terms are zero.
\[
    \implies \vec{b} \cdot (\vec{c} \times \vec{a}) + \vec{c} \cdot (\vec{b} \times \vec{a}) = 0
\]
Now, using the anti-commutativity property of the cross product, we see that \( \vec{a} \times \vec{b} = -\vec{b} \times \vec{a} \).
\[
    \implies \vec{b} \cdot (\vec{c} \times \vec{a}) - \vec{c} \cdot (\vec{a} \times \vec{b}) = 0
\]
\[
    \therefore \vec{c} \cdot (\vec{a} \times \vec{b}) = \vec{b} \cdot (\vec{c} \times \vec{a})
\]
Hence, the equation is always true.

Alternatively, we can show the invariance by evaluating the expressions as a determinant for both sides of the equation.

\newpage
\subsubsection*{(d) \( \vec{a} \times (\vec{b} \times \vec{c}) = (\vec{a} \cdot \vec{c})\ \vec{b} - (\vec{a} \cdot \vec{b})\ \vec{c} \)}

This equation is \underline{always true}.

The quantity \( \vec{a} \times (\vec{b} \times \vec{c}) \) denotes the \textit{vector triple product}.
The given expression is also known as the \textit{triple product expansion}.

We first show that the triple product \( \vec{a} \times (\vec{b} \times \vec{c}) \) lies on the plane containing the vectors \( \vec{b} \) and  \( \vec{c} \).
This follows by observing that the vector \( \vec{p} = \vec{b} \times \vec{c}\) is orthogonal to the plane formed by \( \vec{b} \) and  \( \vec{c} \), and that the vector \( \vec{q} = \vec{a} \times \vec{p} \) is orthogonal to \( \vec{a} \) and \( \vec{p} \).
Thereby using \( \vec{q} \perp \vec{p} \) we can see that \( \vec{q} \) lies on the plane formed by \( \vec{b} \) and  \( \vec{c} \).
The implication of this is that we can express \( \vec{q} \) as a linear combination of \( \vec{b} \) and  \( \vec{c} \), i.e., \( \vec{q} = m\vec{b} + n\vec{c} \), for some scalars \( m \) and \( n \).
\begin{equation*}
    \vec{a} \times (\vec{b} \times \vec{c}) = m\vec{b} + n\vec{c}
\end{equation*}
Taking the dot product on both sides with \( \vec{a} \), we see that
\[
    \vec{a} \cdot (\vec{a} \times (\vec{b} \times \vec{c})) = \vec{a} \cdot (m\vec{b} + n\vec{c})
\]
\[
    \implies 0 = m(\vec{a} \cdot \vec{b}) + n(\vec{a} \cdot \vec{c})
\]
since the dot product is zero between orthogonal vectors.

Without loss of generality, we can assume that \( m = \lambda(\vec{a} \cdot \vec{c}) \) and \( n = -\lambda(\vec{a} \cdot \vec{b}) \) for some scalar \( \lambda \), for the equation to be valid for all vectors \( \vec{a} \), \( \vec{b} \), and \( \vec{c} \).
\begin{equation}
    \vec{a} \times (\vec{b} \times \vec{c}) = \lambda (\vec{a} \cdot \vec{c})\ \vec{b} - \lambda (\vec{a} \cdot \vec{b})\ \vec{c}
    \tag{6.d.1}
    \label{eq:6.d.1}
\end{equation}

Now, comparing the magnitude of the vectors on both sides of the equation, we get
\[
    \lVert \vec{a} \times (\vec{b} \times \vec{c}) \rVert
    = \lVert \lambda (\vec{a} \cdot \vec{c}) \vec{b} - \lambda (\vec{a} \cdot \vec{b}) \vec{c} \rVert
\]
\[
    \implies
    \lVert \lambda (\vec{a} \cdot \vec{c}) \vec{b} - \lambda (\vec{a} \cdot \vec{b}) \vec{c} \rVert
    = \lvert \lambda \rvert \sqrt{ {(\vec{a} \cdot \vec{c})}^2 \lVert \vec{b} \rVert ^2 + {(\vec{a} \cdot \vec{b})}^2 \lVert \vec{c} \rVert ^2 - 2 (\vec{a} \cdot \vec{c})(\vec{a} \cdot \vec{b})(\vec{b} \cdot \vec{c}) }
\]

which is from the cosine rule.
Upon simplifying, we can see that \( \lambda = \pm 1 \), for the equation to be valid all vectors \( \vec{a} \), \( \vec{b} \), and \( \vec{c} \).
Using the right hand convention, we see that \( \lambda =  1 \) is the correct choice.
\begin{equation}
    \therefore \vec{a} \times (\vec{b} \times \vec{c}) = (\vec{a} \cdot \vec{c})\ \vec{b} - (\vec{a} \cdot \vec{b})\ \vec{c}
    \tag{6.d.2}
    \label{eq:6.d.2}
\end{equation}
Alternatively, we can show \( \lambda = 1 \) by taking the dot product with \( \vec{b} \) on both sides of~\eqref{eq:6.d.1}.
\[
    \vec{b} \cdot (\vec{a} \times (\vec{b} \times \vec{c})) = \vec{b} \cdot (\lambda (\vec{a} \cdot \ \vec{c})\ \vec{b} - \lambda (\vec{a} \cdot \vec{b})\ \vec{c})
\]
This involves the scalar triple product, and as seen in the previous subsection (6.c), we have
\[
    \implies
    (\vec{b} \cdot \vec{b})(\vec{a} \cdot \vec{c}) - (\vec{b} \cdot \vec{c})(\vec{a} \cdot \vec{b})
    = \lambda (\vec{a} \cdot \vec{c}) \lVert \vec{b} \rVert^2 - \lambda (\vec{a} \cdot \vec{b})(\vec{b} \cdot \vec{c})
\]
This gives \( \lambda = 1 \) for it to be valid all vectors \( \vec{a} \), \( \vec{b} \), and \( \vec{c} \).

Hence the equation~\eqref{eq:6.d.2} is always true.
